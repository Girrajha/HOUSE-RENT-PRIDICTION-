# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LQHxFDIoNVV0TE37l1K1JJj9MpsrXNCR

# House Price linear regression Project
"""

by using House data we train linear regression and try to predict new outcome



"""## 1. Problem statement"""

by using House data we train linear regression and try to predict new outcome

"""## 2. Libreries import"""

import pandas as pd
import numpy as np

# visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Linear regression import
from sklearn.linear_model import LinearRegression

# train-test-split
from sklearn.model_selection import train_test_split

# evaluation metrics
from sklearn.metrics  import r2_score, mean_squared_error

# vif (no multicolinearity)
from statsmodels.stats.outliers_influence import variance_inflation_factor

# scipy.stats as stats
import scipy.stats as stats    # if we want any statistic model that time we can aslo use scipy.stats or your can use numpy stats function

sklearn >> sckit-learn
linear_model   >> LinearRegression, LogisticRegression

100%  (1000)      # model_selection
training = 80 - 800   # r^2 >>
testing  = 20 - 200

evaluation metrics
best fit line
mse  >> mean_squared_error   ()
r_squared
rmse

"""# 3. Data Gathering"""

# load the csv data by using pandas
#/housing_5000.csv
# y = 'price'                       # dependent column
# x = ['area','bedrooms', 'age']    # independent columns

df = pd.read_csv("/housing_5000.csv")
df.head()

"""# 4. EDA"""

from google.colab import drive
drive.mount('/content/drive')

df.isnull().sum()

df.info()

"""# 5. assumption"""

# assumption1 linearity ( feature vs target linearily correlated)

features = ['area','bedrooms', 'age']

for i in features:
    sns.scatterplot(x=df[i], y=df['price'])
    plt.title(f"{i} vs Pride")
    plt.show()

df.corr()              # r value

df['area'].corr(df['price'])

# r value is 0.999
r>0.7
# area and price have strong correlation

df['bedrooms'].corr(df['price'])
# r values is 0.07
# it is not good

df['age'].corr(df['price'])
# r vlaues is not good

# 2. no multicoliearity

features = ['area','bedrooms', 'age']
vif = 1    >> no correlation
vif = 5    >> moderate correlation
vif = 10   >> highly correlation between 2 or more independent variable

features = ['area','bedrooms', 'age']
x_temp = df[features]         # we create data frame
vif_data = pd.DataFrame()

vif_data['features'] = x_temp.columns
vif_data

features = ['area','bedrooms', 'age']
x_temp = df[features]
vif_data = pd.DataFrame()
vif_data['features'] = x_temp.columns

vif=[variance_inflation_factor(x_temp.values, i) for i in range(0,x_temp.shape[1])]
vif_data["vif"] = vif

vif_data

x_temp.shape[1]

(5000, 3)

features = ['area','bedrooms', 'age']
x_temp = df[features]
vif_data = pd.DataFrame()
vif_data['features'] = x_temp.columns

vif=[variance_inflation_factor(x_temp.values, i) for i in range(0,3)]
vif_data["vif"] = vif

vif_data

"""TRAIN TEST SPLIT"""

x = df[["area","bedrooms","age"]] # 80 train 20 teste
y = df["price"]    # 80 train 20 test

x

y

100
test_size = 0.2  20%
x =t_train , x_test
y = t_train , y_test
random_state= 42 (true ) between 1,99

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

x_train

x_test

"""TRAIN MODEL

fit >> we just store data or we give data fir traing
"""

# TRAIN THE MODEL

model = LinearRegression()
model.fit(x_train, y_train)

